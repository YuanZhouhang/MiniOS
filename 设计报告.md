Lab0: GDB + QEMU 调试64位 RISC-V LINUX

#### 1 实验简介

通过在QEMU上运行Linux来熟悉如何从源代码开始将内核运行在QEMU模拟器上，并且掌握使用GDB
跟QEMU进行联合调试，为后续实验打下基础。

#### 2 实验环境

Ubuntu 20.04 LTS
Docker

#### 3 实验基础知识介绍

略

#### 4 实验步骤

##### 4.1 搭建docker环境

1. docker安装，使用命令

```
curl -fsSL http://get.docker.com | bash -s docker --mirror Aliyun
```

​	这条命令的作用是去指定网址下载docker。

![1601367672879](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601367672879.png)

2. 将用户加入docker组，使用命令

```
sudo usermod -aG
```

​	这条命令的作用是使得用户的ID有权限去操作和使用docker，避免每次都要输入sudo.

![1601367892834](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601367892834.png)

3. 导入docker镜像，使用命令

```
cat oslab.tar | docker import -os:2020
```

​	这条命令的作用相当于解压，这个oslab.tar的压缩包是使用老师给到的链接下载的。

![1601369045930](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601369045930.png)

4. 查看docker镜像，使用命令

```
docker image ls
```

​	这条命令用来查看当前docker镜像的一些信息

![1601376544190](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601376544190.png)

5. 从镜像创建一个容器，使用命令

```
docker run -it oslab:2020 /bin/bash
```

​	容器作为应用包的载体，起到一个由抽象到具体的作用，创建了容器，就可以在容器内进行部署、运行应用		的一系列操作。

注意这里容器的ID为c5d881ac5ce8。

![1601376606220](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601376606220.png)

6. 查看当前运行的容器和所有存在容器，使用命令

```
docker ps
docker ps -a
```

​	当前运行的容器就是启动的容器，容器可以被用户开启或关闭，如果想要查看都有哪些容器被创建可以使用下面的命令。

可以看到这里有个停止的容器的ID为c5d881ac5ce8，这就是刚才我所创建的容器。

![1601376661558](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601376661558.png)

7. 启动处于停止状态的容器和进已运行的容器，使用命令

```
docker start c5d8
docker exec -it -u oslab -w /home/oslab c5 /bin/bash
```

手动开启我们刚刚创建的容器(由于ID的原因，我们使用ID的前四个字符就可以确定该容器)。

进入容器之后，输入界面会变为docker的编辑状态(高亮会不同等区别)，之后就可以开始我们正式的操作。

![1601376785247](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601376785247.png)

##### 4.2 编译linux内核

代码：

```
pwd						//这里相当于显示目前pwd环境变量的内容
/home/oslab
cd lab					//进入lab0目录
export TOP=`pwd`			
export RISCV=/opt/riscv
export PATH=$PATH:$RISCV/bin	

mkdir -p build/linux		//新建了一个目录
make -C linux 0=$TOP/linux \
			CROSS_COMPILE=riscv64-unknown-linux-gnu- \
			ARCH=riscv CONFIG_DEBUG_INFO=y \
			deconfig all -j$(nproc)
												//这行就是具体执行编译linux内核的代码，我的这里改了一下因为我可能建得有问题，build和linux在同一个目录下，因此就把build删去了，不影响使用
```

输出了一长串，这是最后的几行输出：

![1601381420902](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601381420902.png)

##### 4.3使用QEMU运行内核

代码：

```
# qemu-system-riscv64 -nographic -machine virt -kernel linux/arch/riscv/boot/Image \
-device virtio-blk-device,drive=hd0 -append "root=/dev/vda console=ttyS0" \
-bios default -drive file=rootfs.ext4,format=raw,id=hd0 \
-netdev user,id=net0 -device virtio-net-device,netdev=net0
```

输出结果如下(仅截取前面几行)

![1601382777939](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601382777939.png)

之后便运行了虚拟的linux内核：

我可以在这里执行linux环境的命令，虚拟内核也会对相应的命令进行执行。(前面几行是我没搞懂什么意思乱输入的，后面就是我发现这是已经运行的linux虚拟内核之后进行的测试)。

![1601382817196](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601382817196.png)

从虚拟内核中退出：

![1601382937646](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1601382937646.png)

##### 4.4使用gdb对内核进行调试

首先打开terminal1，代码：

```
qemu-system-riscv64 -nographic -machine virt -kernel linux/arch/riscv/boot/Image \
-device virtio-blk-device,drive=hd0 -append "root=/dev/vda ro console=ttyS0" \
-bios default -drive file=rootfs.ext4,format=raw,id=hd0 \
-netdev user,id=net0 -device virtio-net-device,netdev=net0 -S -s
```

这部分代码的意思是，开始运行linux内核，但这个程序受控

![1602075664858](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602075664858.png)

之后打开terminal2，代码：

```
riscv64-unknown-linux-gnu-gdb linux/vmlinux
```

![1602076009050](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602076009050.png)

先进行断点测试：

terminal2代码：

```
target remote localhost:1234		//连接qemu
b start_kernel						//设置断点
continue							//继续执行
quit								//退出gdb
```

这部分代码的意思是，首先连接到qemu，即连接导运行内核的程序，接下来给这个程序设置了一个断点，这个断点编号为1，存储位置也显示在命令行。然后continue即运行程序直到断点位置停止，此时界面上显示了这个断点位置的函数的一些信息。

![1602077014903](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602077014903.png)

由此借助gdb我们可以去给一些程序debug，gdb可以很好的控制一个程序，并获取它的一些信息。

接下来我还尝试了一些其他的gdb指令例如 ：

next:即单步调试

![1602077447674](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602077447674.png)

info:可以输出一些信息，比如这里就可以在程序运行过程中打开register，查看各个register的值。

![1602077481629](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602077481629.png)

frame:可以切换函数的栈帧，此时把编号为1的栈帧作为当前的栈帧，接着我们还可以打印该栈帧的信息。

![1602077503133](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602077503133.png)

finish：结束函数调用，回到调用点。

![1602078746058](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602078746058.png)

还有一些指令例如display\print可以输出程序特定变量的值，这里由于不知道源码无法测试。

另外，backtrace(查看函数的调用的栈帧和层级关系)的执行不成功，是因为backtrace和frame密切相关，经常出现frame did  not save the PC，我认为跟函数的选择和断点的设置都有关，这个命令就不再测了。



## Lab 1:Boot Kernel

### 1 实验简介

​	学习RISC-V相关知识，Makefile相关知识，编写head.S实现bootloader的功能，并利用Makefile来完成对整个工程的管理。

### 2 实验环境

​	Docker in Lab0

### 3 实验步骤

#### 3.1 搭建实验环境

首先创建新的容器，容器，并进入容器。

![1602135743809](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602135743809.png)

![1602135769974](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602135769974.png)

观察容器是否建立映射成功，在本机创建的文件，容器内对应位置也存在，可以看出映射建立正确。

![1602135873199](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602135873199.png)

#### 3.2 编写Makefile

##### 3.2.1 组织文件结构

接下来，依次创建对应位置的目录和文件，如下

![1602136318109](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602136318109.png)

![1602136395656](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602136395656.png)

![1602136513092](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602136513092.png)

最终结果：

可以看出组织文件结构也正确

![1602136607468](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602136607468.png)

![1602136665579](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602136665579.png)

##### 3.2.2 编写各项目录 Makefile

顶层Makefile：

功能：

设置各种变量，并传递给下层的makefile，其中$(GCC)用于编译，$(LD)用于链接，$(OBJCOPY)用于生成Image文件，$(CFLAG)是使用$(GCC)编译的时候的参数。

之后调用下层的makefile

我还设置了伪目标clean用于清除，run用于运行内核，debug用于测试内核

```makefile
export
CROSS_= riscv64-unknown-elf-
AR=${CROSS_}ar
GCC=${CROSS_}gcc
LD=${CROSS_}ld
OBJCOPY=${CROSS_}objcopy

ISA ?= rv64imafd
ABI ?= lp64

INCLUDE = -I ../include
CF = -O3 -march=$(ISA) -mabi=$(ABI) -mcmodel=medany -ffunction-sections -fdata-sections -nostartfiles -nostdlib -nostdinc -static -lgcc -Wl,--nmagic -Wl,--gc-sections
CFLAG = ${CF} ${INCLUDE}

subsystem:
	cd init && make
	cd arch/riscv/kernel && make
	cd arch/riscv && make

.PHONY : clean
clean: 
	cd init && make clean
	cd arch/riscv/kernel && make clean
	cd arch/riscv && make clean

.PHONY : run
run:
	qemu-system-riscv64 -nographic -machine virt -kernel arch/riscv/vmlinux

.PHONY : debug
debug:
	qemu-system-riscv64 -nographic -machine virt -kernel arch/riscv/vmlinux

```

init目录下Makefile:

功能：使用riscv64下的gcc编译文件生成main.o和test.o

```makefile
all : main.o test.o
.PHONY : all

vpath test.h ../include

main.o : main.c test.h
	$(GCC) -c main.c $(CFLAG)
test.o : test.c test.h
	$(GCC) -c test.c $(CFLAG)

.PHONY : clean
clean : 
	rm main.o test.o

```

arch/riscv目录下Makefile:

功能：链接目标文件生成内核vmlinux，接着生成Image文件和显示符号表。

```makefile
all : vmlinux Image System.map
.PHONY : all

vpath vmlinux.lds ./kernel
vpath test.o ../../init
vpath main.o ../../init
vpath head.o ./kernel

vmlinux : main.o test.o head.o vmlinux.lds
	$(LD)  ../../init/main.o ../../init/test.o kernel/head.o -T kernel/vmlinux.lds -o vmlinux

Image : vmlinux
	$(OBJCOPY) -O binary vmlinux Image --strip-all

System.map : vmlinux
	nm vmlinux

.PHONY : clean
clean : 
	rm vmlinux Image
	

```

arch/riscv/kernel目录下Makefile:

功能：使用riscv64下的gcc编译head.S生成head.o

```makefile
head.o : head.S
	$(GCC) -c head.S $(CFLAG)

.PHONY : clean
clean :
	rm head.o

```



#### 3.3 编写head.S

head.s代码：

```asm
.global _start
.align 4
_start:
#关闭machine mode中断
addi t0, zero, 0x01
slli t0, t0, 0x07
addi t0, t0, 0x08
csrc mstatus, t0

csrw mie, zero
csrw mip, zero	

#设置machine mode异常处理地址
la t0, mexcep_vet
csrw mtvec, t0

#使cpu从machine mode切换到supervisor mode

addi t0, zero, 0x1		
slli t0, t0, 0xB		
csrs mstatus, t0
slli t0, t0, 0x1
csrc mstatus, t0
la t0, Continue
csrw mepc, t0
mret
	
Continue:
#设置supervisor mode异常处理地址
la t0, sexcep_vet
csrw stvec, t0

#设置c语言调用栈环境
la sp, stack_top

#跳转到main.c中给出的start_kernel函数
j start_kernel

.global mexcep_vet
.align 4
mexcep_vet:

.global sexcep_vet
.align 4
sexcep_vet:


```

代码解释：

​	这段代码的核心是mstatus寄存器，该寄存器是一个状态寄存器，mstatus里由控制中断和控制权限模式的bit位，配合mret指令的使用可以改变当前hart状态。mret的功能是1.可以将mepc的值赋给pc寄存器；2.根据mstatus的MPP[1:0]位改变当前权限模式(MPP[1:0]==01时改为supervisor mode)；3.根据mstatus的mie位设置当前是否允许中断；4.将mstatus的mpie位赋给mie位。因此通过mstatus和mret配合就可以实现模式转换和关闭中断。

关闭中断：只需要设置mstatus的mie和mpie位，在之后的mret指令执行之后中断就已经关闭。

设置m模式异常处理地址：由于该程序不会引发异常，异常处理地址可以任意选取，即不对异常进行处理，我这里就随便设置了异常处理地址。

切换模式：head.S这段代码运行开始的时候处于machine mode，此时由于权限很高，基本上什么都可以做，而如果什么都不做就mret，那么不仅pc无法跳转到正确的位置，同时由于权限模式已经更改为user mode，之后想要对stvec赋值也是无权限的，因此在切换模式之前要先设置mepc和mstatus的MPP位。

设置s模式异常处理地址：同理，值得一提的是，如果MPP设置错误，无法正确的跳转到s模式而是进入了u模式的话，这里就无法对stvec进行写入。

设置c语言调用栈环境：堆栈地址在vmlinux.lds中给出，这里只需要将该地址赋值给sp寄存器即可。

跳转start_kernel：直接用标签跳转即可。

#### 3.4 编译及测试

最终结果：

![1602730129896](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602730129896.png)

以下为debug过程，由于debug过程比较长，我主要精力放在解决问题，就没有过多截图。由于我的汇编一开始写的有问题，不得已只能使用实验一的方式进行debug。

![1602676228267](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1602676228267.png)



## Lab2 RISC-V64时钟中断处理

### 1 实验简介

​	学习在RISC-V上的异常处理相关机制，以时钟中断为例，编写时钟中断处理函数。

### 2 实验环境

​	Docker Image

### 3 背景知识

#### supervisor mode下时钟中断处理流程(自我理解版，着重写了我自己认为的重要的细节)

##### 1. 设置时钟中断委托

需要将mideleg[5]置位，mideleg是机器中断委托CSR寄存器，控制将哪些中断委托给S模式，且只有当mideleg对应位置置位时才可以读写sie，sip寄存器，否则无法写入。

之后需要置位mstatus[mie],sstatus[sie],mie[mtie],sie[stie]。其中mstatus和mie确保M模式下时钟中断条件满足的时候中断可以发生，同理sstatus和sie确保S模式下中断打开。

接下来当mtimecmp小于mtime时，硬件自动将mip[mtip]置位，此时满足条件(mstatus[mie]=1, mie[mtie]=1, mip[mtip=1])发生中断，跳转至M模式异常处理函数处理时钟中断。

##### 2. M模式异常处理函数处理时钟中断

首先需要判断是否是哪种异常，由于本实验只需要实现时钟中断和S模式ecall两种异常，因此异常处理函数也只需要应对这两种情况即可，而查阅相关资料可知，ecall产生的是一个同步异常，因此可以通过判断mcause最高位取分这两种异常(其实就是区分中断和同步异常)。

判断是时钟中断之后，由于要将该异常交给S模式处理，因此置位mip[stip]并清零mie[mtie]。这里置位mip[stip]其实目的是置位sip[stip]，由于子集关系置位mip[stip]和sip[stip]是等价的清零mie[mtie]使得之前M模式中断条件不满足，于是无法产生时钟中断，而此时由于sip[stip]置位，S模式的时钟中断条件满足，于是会触发S模式下的时钟中断。之后由于要使用S模式来处理中断，应将权限改为S模式(mstatus[MPP]=01)，最后mret返回中断触发指令位置。

##### 3. S模式异常处理函数处理时钟中断

S模式处理比较简单，调用一个用c语言实现的格式化输出函数即可，之后由于要将状态恢复，于是使用ecall进入M模式的异常处理函数。

##### 4. M模式异常处理函数处理ecall同步异常

判断是同步异常之后，M模式将mtimecmp增加，此时硬件会将mip[mtip]清零，于是此时之后恢复mie[mtie]不会出发M模式下的时钟中断。然后将mie[mtie]清零，此时还应当确保mstatus[mie]和sstatus[sie]处于置位状态，因为马上就要回到中断出发的指令，为了保证之后中断还可以正常触发，需要确保前置条件满足，最后将权限返回为M模式(mstatus[MPP]=11)。我在这里手动确认了一下。这些都处理完之后就可以mret返回ecall触发地址即S模式异常处理函数中了(在这一步需要mepc+4，原因我在后面思考题里会写)。

##### 5. 回到S模式异常处理函数

实际我是在这里确保了mstatus和sstatus的状态，因为这更符合S模式时钟中断处理函数的功能。在确保所有状态都正确的情况下，我们还需要确定如何从S模式返回，一种方法是使用mret，但由于在该函数中使用了ecall，mepc被置位过了，要在这里将mepc置为先前的值，就要把mepc提前保存，同时mret还会修改mstatus的值，最终我没有使用这种方法，我使用了ret的方法，这样我可以在异常处理函数中确保mstatus是我想要的值。而ret需要的就是将原先mepc的值取出放入ra寄存器里，之后便可以返回中断发生指令位置。

### 4 实验步骤

#### 4.1 环境搭建

##### 4.1.1 建立映射

略

##### 4.1.2 组织文件结构

编译完成后文件结构：

![1603856495443](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1603856495443.png)

编译开始前文件结构：

![1603856607612](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1603856607612.png)

另：我将不会再后续实验报告里附上Makefile代码，因为不重要，我确保其能用。

#### 4.2 vmlinux.lds修改 

```
OUTPUT_ARCH( "riscv" )
ENTRY( _start )
BASE_ADDR = 0x80000000;
SECTIONS
{
  . = BASE_ADDR;
  .text : { *(.text.init)
            *(.text.entry)
            *(.text)
           }
  .rodata : { *(.rodata) }
  .data : { *(.data) }
  .bss : { *(.bss) }
  . += 0x8000;
  stack_top = .;
  _end = .;
}
```

这里修改的是.text，因为要确保head.S的代码最先执行(内含初始化)。

#### 4.3 head.S

##### 4.3.1 head.S源码

```assembly
.section	.text.init
.global _start
.align 4
_start:
//初始化bss段
	la t0, _end
	addi t0, t0, 0x02
	sw zero, 0(t0)
	sw zero, 4(t0)
	
	li s1, 0x200bff8	//mtime
	li s2, 0x2004000	//mtimecmp
//初始化mtimecmp寄存器
	//取mtime的值
	lw t0, 0(s1)
	//+=100000
	addi t1, zero, 0x01
	slli t1, t1, 0x14
	add t0, t0, t1
	//写入mtimecmp
	sw t0, 0(s2)
//设置时钟中断委托
	//mideleg[5]
	addi t0, zero, 0x01
	slli t0, t0, 0x05
	csrs mideleg, t0
	//mstatus[mie]
	addi t0, zero, 0x01
	slli t0, t0, 0x03
	csrs mstatus, t0
	//sstatus[sie]
	addi t0, zero, 0x01
	slli t0, t0, 0x01
	csrs sstatus, t0
	//mie[mtie]
	addi t0, zero, 0x01
	slli t0, t0, 0x07
	csrs mie, t0
	//sie[stie]
	addi t0, zero, 0x01
	slli t0, t0, 0x05
	csrs sie, t0

//设置异常处理地址
	la t0, trap_m
	csrw mtvec, t0	
	la t0, trap_s
	csrw stvec, t0

//设置栈指针
	la sp, stack_top

//进入start_kernel函数
	j start_kernel



.global trap_m
.align 4
trap_m:
	//判断是时钟中断-mcause
	csrr t0, mcause
	srli t0, t0, 0x0f
	beq t0, zero, _ecallfromS
	
	//保存寄存器mstatus,sstatus,mepc,mcause,s1,s2
	addi t0, zero, 0x1c
	sub sp, sp, t0
	csrr t0, mstatus
	sw t0, 24(sp)
	csrr t0, sstatus
	sw t0, 20(sp)
	csrr t0, mepc
	sw t0, 16(sp)
	srli t0, t0, 0x20
	sw t0, 12(sp)
	csrr t1, mcause
	sw t1, 8(sp)
	sw s1, 4(sp)
	sw s2, 0(sp)
	
//	jal m_trans
	
	//清除mie[mtie]
	addi t0, zero, 0x01
	slli t0, t0, 0x07
	csrc mie, t0
	//置位mip[stip]
	addi t0, zero, 0x01
	slli t0, t0, 0x05
	csrs mip, t0 
	
	//切换至S mode
	addi t0, zero, 0x01
	slli t0, t0, 0x0c
	csrc mstatus, t0
	
	//取堆栈
	lw s2, 0(sp)
	lw s1, 4(sp)
	lw t1, 8(sp)
	
	addi sp, sp, 0x0c
	
	csrw mcause, t1
	
	mret	
	
//判断是ecall
_ecallfromS:

	addi t0, zero, 0x08
	sub sp, sp, t0
	sw s1, 4(sp)
	sw s2, 0(sp)

//	jal m_ecall

	//清零sip[stip]	
	addi t0, zero, 0x01
	slli t0, t0, 0x05
	csrc mip, t0

	//mtimecmp+=100000
	lw t0, 0(s1)
	addi t1, zero, 0x01
	slli t1, t1, 0x14
	add t0, t0, t1
	sw t0, 0(s2)
	//置位mie[mtie]
	addi t0, zero, 0x01
	slli t0, t0, 0x07
	csrs mie, t0
	
	//切换回M mode
	addi t0, zero, 0x01
	slli t0, t0, 0x0c
	csrs mstatus, t0
	
	lw s2, 0(sp)
	lw s1, 4(sp)
	
	addi sp, sp, 0x08
	
	csrr t0, mepc
	addi t0, t0, 0x04
	csrw mepc, t0

	add t0, t0, zero
	add t0, t0, zero
	
	mret
```

##### 4.3.2 head.S讲解

流程见第三部分。

其实大部分的CSR寄存器位设置根据实验手册里的讲解做就可以了，但是我最终的实现与实验手册还是有一点不一样，总共由两个部分，在这里着重讲一下。

第一个是权限转换，手册里没有提到在将时钟中断委托给S模式之前需要将权限转换，但我在实际的测试中发现，如果在M模式异常处理函数退出之前不改为S权限模式，那么S模式的异常处理函数用于不会被调用。因此我就尝试在这里将mstatus的MPP位改成01(S mode)，之后S模式的异常处理函数就可以被调用了。

另一个不一样的地方与S模式的异常处理函数有关，我会写在下面一个部分。

#### 4.4 entry.S

##### 4.4.1 entry.S源码

```assembly
.section	.text.entry
.global trap_s
.align 4
trap_s:
//	jal s_trap
	
	//调用strap.c实现的strap函数
	jal strap

	ecall
	
_afterecall:
	lw t0, 0(sp)
	lw t1, 4(sp)
	lw t2, 8(sp)
	lw t3, 12(sp)
	slli t0, t0, 0x20
	slli t1, t1, 0x20
	srli t1, t1, 0x20
	add ra, t0, t1
	
	csrw sstatus, t2
	csrw mstatus, t3
	//重置mstatus[mie]
	addi t0, zero, 0x01
	slli t0, t0, 0x03
	csrs mstatus, t0
	
	addi sp, sp, 0x10

	ret	
```

##### 4.4.2 entry.S讲解

总流程见第三章节。

上面说到还有一个与实验手册不一致的地方，是在M mode中，因为在S mode中由于使用了ecall，因此很多的寄存器都可能会发生变化，因此就需要上下文切换，但我在实际debug的时候发现，在S模式还未ecall之前，由于处在S模式的关系，我没法对mstatus进行读取。这里有两个办法，一个是切换回M模式，但我觉得这样就违反了我中断委托的想法，于是我只能选择另一个不太安全的方法，即在M模式时钟中断处理函数中存mstatus，再在S模式时钟中断处理函数里取mstatus。

其实我认为这不是最好的办法，一定还有更好的办法，不过这个办法暂时不会出现bug就是了。

#### 4.5 strap.c

##### 4.5.1 strap.c源码

```c
#include "put.h"

int i;
int loop;

int strap()
{
	const char *msg = "[S] Supervisor Mode Timer Interrput ";
	const char *end = "\n";
	loop++;

	
	if(loop >= 10){
		loop = 0;
    		puts(msg);
		puti(i);
		puts(end);
		i++;
	}
    return 0;
}

void s_trap(){
	const char *msg = "supervisor mode.\n";
	puts(msg);
}

void m_trans(){
	const char *msg = "machine mode trans.\n";
	puts(msg);
}

void m_ecall(){
	const char *msg = "machine mode ecall.\n";
	puts(msg);
}

```

##### 4.5.2 strap.c讲解

这里strap函数是S模式最终调用的格式化输出函数，其他的s_trap,m_trans,m_ecall是我在调试的时候使用的实出函数，用于在run模式确认程序进行到哪里了(不过后面一直用debug模式，单步调试的时候gdb就会告诉我位于哪个函数里，这几个函数有点鸡肋了)。

strap函数主要是使用put.h中实现的puti()和puts()两个函数来分别实现输出整数和输出字符串的。同时这里还有两个全局变量i和loop，其中i表示的是Supervisor Mode Time Interrupt后面的编号，而loop是用来调整输出速率的(实验指导上写的100000)，但我实际测试下来发现如果loop设置为100000的话太慢了，根本等不到输出，改成10刚刚好，这样的话就是几乎每一秒输出一行""[S] Supervisor Mode Timer Interrupt"。

这里也会设计bss段的初始化，其实最后初始化的就是i和loop这两个未初始化的全局变量，这个我在后面思考题部分也会讲。

#### 4.6 最终结果

![1603857406435](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1603857406435.png)

![1603857489542](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1603857489542.png)

![1603857517030](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1603857517030.png)

![1603857529031](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1603857529031.png)

![1603857546310](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1603857546310.png)

写报告的时候发现我这里手快把Interrupt打错了，linux里面没有单词拼错检索......请忽略这个错误吧......





# 实验三：RISC-V64简单的进程调度实现

## 1 实验简介

​	结合课堂所学习的相关内容，在上一实验实现中断的基础上进一步地实现简单的进程调度

## 2 实验环境

​	Docker Image

## 3 背景知识

略

## 4 实验步骤

### 4.1 环境搭建 

#### 4.1.1 建立映射

​	同lab2的文件夹映射方法。

#### 4.1.2 组织文件结构

![1606130610680](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1606130610680.png)

### 4.2 rand.h rand.c介绍

略

### 4.3 sched.h数据结构定义

略

### 4.4 sched.c进度调度功能实现

#### 4.4.1 在中断处理中添加保存epc的指令	

​	这里有一个很大的问题，epc寄存器实际上保存的是调度发生的地址，而这个地址是与进程相关的，按道理来说这个地址应该被保存在与进程相关的数据结构中。然而我读代码发现sched.h中并没有提供给我们保存这个epc的位置，也就是说我们要不就修改sched.h的数据结构，给每个task都增加保存epc的变量，要么就没办法实现严格的调度算法，我选择了后者，因此我这里并没有保存epc，具体做法见下文。

#### 4.4.2 实现task_init()，由于这部分两种调度算法相差不大因此就一起写了

```c
	//创建0号进程
	task[0] = (struct task_struct*)(0x80010000);
	task[0]->state = TASK_RUNNING;
	task[0]->counter = 0;
	task[0]->priority = 5;
	task[0]->blocked = 0;
	task[0]->pid = 0;
	task[0]->thread.sp = 0x80011000;
	current = task[0];
	
	//创建1号进程
	task[1] = (struct task_struct*)(0x80011000);
	task[1]->state = TASK_RUNNING;
	task[1]->counter = rand();
	task[1]->priority = 5;
	task[1]->blocked = 0;
	task[1]->pid = 1;
	task[1]->thread.sp = 0x80012000;
	task[1]->thread.ra = (unsigned long long)&init_ra;
	msg = "[PID = 1] Process Create Successfully! counter = ";
	puts(msg);
	puti(task[1]->counter);
	msg = "\n";
	puts(msg);
```

​	这里是task_init()中主要的内容，task_init()函数主要的功能是进行所有所需的4+1个进程的结构体的初始化。初始化内容包括结构体地址、结构体运行状态、剩余运行时间、优先级、进程ID、进程分配寄存器sp、ra的值。

​	注意这里比较重要的是处理进程寄存器sp、ra。我们可能会直观的认为这里的ra是进程发生调度的地址，然而正如我前面所提到，调度发生位置的地址被存在了sepc寄存器当中，这里的ra实际上只起到一个定位函数的作用，这里的ra可以看到我将其定为到了一个名为init_ra的标签除，这个标签是一个汇编函数，它的作用是将dead_loop函数的首地址赋值给sepc，然后我将在switch_to()函数中使用ret命令让程序强制跳转到init_ra函数中，将sepc赋值，再使用sret返回dead_loop首地址。

​	另一个值得注意的点是sp寄存器，由于每个进程都有自己的地址空间，每个进程也都有自己的栈，因此再发生调度后，sp的地址是变的，因此在发生调度前后存取堆栈都要非常小心，尤其是c语言函数在执行的过程中会自动补充状态保存的内容，如果在c语言内部改变了sp，就不能让c语言通过正常(return)的方式退出，否则会发生栈的错误访问。

#### 4.4.3 实现do_timer()

##### 4.4.3.1 短作业有限非抢占式调度

```c
void do_timer(void){

	static int x = 0;
	//外层循环用来调整输出速度
	if(x == 0){
        //输出信息
		puts("[PID = ");
		puti(current->pid);
		puts("] Context Calculation: counter = ");
		puti(current->counter);
		puts("\n");
		
        //如果剩余时间为0，则发生调度
		if(current->counter <= 1){
			current->counter = 0;
			schedule();
		}
        //否则剩余时间减一
		else{
			current->counter --;
		}
		x++;
	}
	else if(x == 99){
		x = 0;
	}
	else{
		x++;
	}
	return;
}
```

​	短作业优先非抢占比较简单，只有运行时间为0的时候才会发生调度。同时这也产生一个问题是，这种方式比较难以出现发生调度前后进程号相同的情况，这也在我后面写优先级抢占式算法的时候遇到的问题埋下了伏笔。

##### 4.4.3.2 优先级抢占式算法

```c

```

​	优先级抢占式调度略麻烦一些，这个算法在每一次执行完一个单位时间后都要进行调度，并且在时间用完之后会将时间重置。而且每一次调度完之后优先级又会更新，也就使得这个算法发生调度之后发现进程号没变或者执行调度函数，判断后发现不需要调度这样的情况频繁出现，这也就引发了我在最后阶段debug的时候的一个问题，我在下面会讲到。

#### 4.4.4 实现schedule()

##### 4.4.4.1 短作业优先非抢占式算法

```c
void schedule(void){
	int max = 0;
	int min = 1000;
	struct task_struct * temp;
	
	//获得最大最小的counter
	for(int i=4; i>0; i--){
		if(task[i]->counter > max)
			max = task[i]->counter;
		if(task[i]->counter < min && task[i]->counter > 0){
			temp = task[i];
			min = task[i]->counter;
		}
	}
	
	//全部为0，需要重新分配counter
	if(max == 0){
		for(int i=1; i<5; i++){
			task[i]->counter = rand();
			puts("[PID = ");
			puti(i);
			puts("] Reset counter = ");
			puti(task[i]->counter);
			puts("\n");
		}	
		
		//分配完重新找到最小counter的task
		min = 1000;
		for(int i=4; i>0; i--){
			if(task[i]->counter < min && task[i]->counter > 0){
				temp = task[i];
				min = task[i]->counter;
			}
		}
		
		switch_to(temp);
	}
	//正常转换
	else{
		switch_to(temp);
	}
	
}
```

这里没什么好说的，按照手册写就不会错。

##### 4.4.4.2 优先级抢占式算法

```c
void schedule(void){
	int minP = 10;
	int minPminT = 10;
	struct task_struct * temp;
	
	//遍历所有进程，获得最优先
	for(int i=4; i>0; i--){
		if(task[i]->priority < minP){
			minP = task[i]->priority;
			minPminT = task[i]->counter;
			temp = task[i];
		}
		else if(task[i]->priority == minP && task[i]->counter < minPminT){
			minPminT = task[i]->counter;
			temp =  task[i];
		}
	}
	
	switch_to(temp);
	
	return;
}
```

#### 4.4.5 实现switch_to()，这个函数两种算法区别也不大，也一起写了

```c
void switch_to(struct task_struct *next){
	if(next != current){
		//输出进程转换
		puts("[!] Switch from task ");
        	puti(current->pid);
        	puts(" to task ");
        	puti(next->pid);
        	puts(", prio: ");
        	puti(next->priority);
        	puts(", counter: ");
        	puti(next->counter);
        	puts("\n");
        	
        	puts("tasks' priority changed\n");
		for(int i=1; i<5; i++){
			task[i]->priority = rand();
			puts("[PID = ");
			puti(i);
			puts("] counter = ");
			puti(task[i]->counter);
			puts(" priority = ");
			puti(task[i]->priority);
			puts("\n");
		}
        	
        	//寄存器保存
        	__asm__ __volatile__(
        		"mv	%[dsp], sp\n\
        		 mv	%[ds0], s0\n\
        		 mv	%[ds1], s1\n\
        		 mv	%[ds2], s2\n\
        		 mv	%[ds3], s3\n\
        		 mv	%[ds4], s4\n\
        		 mv	%[ds5], s5\n\
        		 mv	%[ds6], s6\n\
        		 mv	%[ds7], s7\n\
        		 mv	%[ds8], s8\n\
        		 mv	%[ds9], s9\n\
        		 mv	%[ds10], s10\n\
        		 mv	%[ds11], s11"
        		:[dsp]"=r"(current->thread.sp),[ds0]"=r"(current->thread.s0),
        		 [ds1]"=r"(current->thread.s1),[ds2]"=r"(current->thread.s2),[ds3]"=r"(current->thread.s3),
        		 [ds4]"=r"(current->thread.s4),[ds5]"=r"(current->thread.s5),[ds6]"=r"(current->thread.s6),
        		 [ds7]"=r"(current->thread.s7),[ds8]"=r"(current->thread.s8),[ds9]"=r"(current->thread.s9),
        		 [ds10]"=r"(current->thread.s10),[ds11]"=r"(current->thread.s11)
        		 ::
        	);
        	
        	current->thread.ra = (unsigned long long)&init_ra;
        	regtemp = current->thread.ra;

		//current赋值
		current = next;
		
		regtemp = current->thread.ra;
		
		//寄存器取出
		__asm__ __volatile__(
        		"mv	ra, %[sra]\n\
        		 mv	sp, %[ssp]\n\
        		 mv	s0, %[ss0]\n\
        		 mv	s1, %[ss1]\n\
        		 mv	s2, %[ss2]\n\
        		 mv	s3, %[ss3]\n\
        		 mv	s4, %[ss4]\n\
        		 mv	s5, %[ss5]\n\
        		 mv	s6, %[ss6]\n\
        		 mv	s7, %[ss7]\n\
        		 mv	s8, %[ss8]\n\
        		 mv	s9, %[ss9]\n\
        		 mv	s10, %[ss10]\n\
        		 mv	s11, %[ss11]\n\
        		 ret"
        		::[sra]"r"(current->thread.ra),[ssp]"r"(current->thread.sp),[ss0]"r"(current->thread.s0),
        		  [ss1]"r"(current->thread.s1),[ss2]"r"(current->thread.s2),[ss3]"r"(current->thread.s3),
        		  [ss4]"r"(current->thread.s4),[ss5]"r"(current->thread.s5),[ss6]"r"(current->thread.s6),
        		  [ss7]"r"(current->thread.s7),[ss8]"r"(current->thread.s8),[ss9]"r"(current->thread.s9),
        		  [ss10]"r"(current->thread.s10),[ss11]"r"(current->thread.s11)
        		 :"ra","sp","s0","s1","s2","s3","s4","s5","s6","s7","s8","s9","s10","s11"
        	);

	}
	else{
		//输出进程转换
		puts("[!] Switch from task ");
        	puti(current->pid);
        	puts(" to task ");
        	puti(next->pid);
        	puts(", prio: ");
        	puti(next->priority);
        	puts(", counter: ");
        	puti(next->counter);
        	puts("\n");
        	
        	puts("tasks' priority changed\n");
		for(int i=1; i<5; i++){
			task[i]->priority = rand();
			puts("[PID = ");
			puti(i);
			puts("] counter = ");
			puti(task[i]->counter);
			puts(" priority = ");
			puti(task[i]->priority);
			puts("\n");
		}
		
		current->thread.ra = (unsigned long long)&init_ra;
		
		__asm__ __volatile__(
        		"mv	ra, %[sra]\n\
        		 mv	sp, %[ssp]\n\
        		 mv	s0, %[ss0]\n\
        		 mv	s1, %[ss1]\n\
        		 mv	s2, %[ss2]\n\
        		 mv	s3, %[ss3]\n\
        		 mv	s4, %[ss4]\n\
        		 mv	s5, %[ss5]\n\
        		 mv	s6, %[ss6]\n\
        		 mv	s7, %[ss7]\n\
        		 mv	s8, %[ss8]\n\
        		 mv	s9, %[ss9]\n\
        		 mv	s10, %[ss10]\n\
        		 mv	s11, %[ss11]\n\
        		 ret"
        		::[sra]"r"(current->thread.ra),[ssp]"r"(current->thread.sp),[ss0]"r"(current->thread.s0),
        		  [ss1]"r"(current->thread.s1),[ss2]"r"(current->thread.s2),[ss3]"r"(current->thread.s3),
        		  [ss4]"r"(current->thread.s4),[ss5]"r"(current->thread.s5),[ss6]"r"(current->thread.s6),
        		  [ss7]"r"(current->thread.s7),[ss8]"r"(current->thread.s8),[ss9]"r"(current->thread.s9),
        		  [ss10]"r"(current->thread.s10),[ss11]"r"(current->thread.s11)
        		 :"ra","sp","s0","s1","s2","s3","s4","s5","s6","s7","s8","s9","s10","s11"
        	);
	}
	return;
}
```

​	这里有两个要点我要说明一下，第一是我的算法如何实现调度返回。由于sepc没办法保存，我没办法定位准确的发生调度的地址，因此我采取了一种简化的办法(只适用于当前实验dead_loop的情况，如果后续实现要接着当前实验做我希望老师能在上课时间讲解一下这个实验)。我的简化办法是，每次都通过ra跳转至init函数，该函数将sepc定位到dead_loop的首地址，由于dead_loop函数只有一行指令，可以看作跳转到了正确的调度发生位置。我的这种办法只适用于当前的实验，原则上来讲其实是不对的，不过我考虑到该实验重点是调度算法，并且实在不知道该如何存储sepc，最终使用了该办法。

​	第二个是我在写的时候，一开始依照手册的提示，只写了if(next != current)的情况，但由于这里一定不能使用正常的return返回，我在后面测试的时候就遇到了无法通过的错误，后来我才意识到，补上了if(next == current)的判断下的处理，才得以通过。

### 4.5 编译及测试

#### 短作业有限非抢占式

![1606134654563](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1606134654563.png)

#### 优先级抢占式算法

![1606134440933](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1606134440933.png)



## HW4

### 1 实验简介

结合课堂学习的页式内存管理以及虚拟内存的相关知识，尝试在已有的程序上开启 MMU 并实现页映射，保证之前的进程调度能在虚拟内存下正常运行。

### 2 实验环境

在实验 3 的基础上进行（Docker Image）

### 3 背景知识

略

### 4 实验步骤

#### 4.1 环境搭建

![1607855991885](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1607855991885.png)

#### 4.2 创建映射

vm.c中两个函数分别为paging_init()，用于设定多个映射，映射初始化。create_mapping()用于建立实际的映射关系。

##### paging_init():

```c
void paging_init(){

	create_mapping(pgtbl1,         0x10000000, 0x10000000, 0x1000000, 0xf);
	create_mapping(pgtbl1,         0x80000000, 0x80000000, 0x1000000, 0xf);
	create_mapping(pgtbl1, 0xffffffe000000000, 0x80000000, 0x1000000, 0xf);	
	return;

}
```

分别对三个虚拟地址建立映射，虚拟地址分别为0x10000000(建立等值映射)，0x80000000(建立等值映射，用途是在MMU打开但地址仍然识别为旧地址和M模式的情况)，0xffffffe000000000(建立到物理地址0x80000000的映射)。

##### create_mapping():

```c
uint64* pgtbl1 = (END + 0x080000000 );
uint64* pgtbl2 = (END + 0x080140000 );
uint64* pgtbl3 = (END + 0x080180000 );

void create_mapping(uint64* pgtbl, uint64 va, uint64 pa, uint64 sz, int perm){
	
	int VPN[3];
	VPN[2] = (va >> 30) & 0x1ff;
	VPN[1] = (va >> 21) & 0x1ff;
	VPN[0] = (va >> 12) & 0x1ff;
	int offset = va & 0xfff;

	//第一层页表
	uint64 * page_address = pgtbl + VPN[2];
	
	*page_address = (uint64)(((uint64)pgtbl2 >> 12 << 10) + 0x0001);


	//第二层页表	
	page_address = pgtbl2 + VPN[1];
	
	for(int i=0; i<8; i++){
		*(page_address + i) = (((uint64)pgtbl3 + i*512*8) >> 12 << 10) + 0x0001;
		
	}

	pgtbl2 += 0x1000;
	
	//第三层页表
	page_address = pgtbl3;
	int PERM = perm;
	
	for(int i=0; i<8; i++){
		for(int j=0; j<512; j++){
			*(page_address + i*512 + j) = (uint64)((((pa >> 12) + (i*512+j)<<12) >> 12 << 10) + PERM);
		}	
	}
	
	pgtbl3 += 0x10000;
	
	return;
}
```

首先，页表的映射流程如下：根据satp得到第一层页表的根地址，与VPN[2]结合得到相应虚拟地址的entry，然后从该entry中得到下一级页表的根地址，再与VPN[1]结合得到相应虚拟地址的entry，然后从该entry中得到下一级页表的根地址，因为总共只有三级，到这里已经找到了页表的最后一级。然后与VPN[0]结合得到entry，这个entry里面保存的就是物理地址的根地址，再与虚拟地址的offset运算的到最终的物理地址。

因此我们在设置页表映射的时候，由于虚拟地址和物理地址都是固定的，因此大部分内容都是固定的，物品们唯一要手动设置的是每一层页表的根地址。因此首先设定三层页表的首地址pgtbl1,pgtbl2,pgtbl3。然后根据虚拟地址的结构去页表对应地址写下一层根页表地址就可以了，这里就不详细描述逻辑了，但是我可以展示一个最后的页表映射结果。

![1607857996611](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1607857996611.png)

这是我在debug模式下，在paging_init()函数执行完毕之后的结果，我希望查询0x80000000的虚拟地址是否建立了 去往0x80000000的物理地址的映射，于是查看0x80007010（第一层页表根地址+VPN[2]），将结果右移10位再左移12位，得到第二层页表根地址0x8014f000，第二第三层页如此操作，最终得到最后的entry里结果位0x2000000f，再经过移位得到0x80000000，再加上offset得到0x80000000，验证正确。

除了0x80000000之外我还验证了其他几个地址，都是正确的，在这里就不一一演示了。

#### 4.3 修改head.S

##### 4.3.1 修改系统启动部分代码

```assembly
//satp清零
add t0, zero, zero
csrw satp, t0

//调用paging_init函数进行页表设置
call paging_init
_afterpageinit:
//设置satp的值为根页表首地址
addi t0, zero, 0x8
slli t0, t0, 60
li t1, 0x80007
add t0, t0, t1
csrw satp, t0
//设置stvec的值为虚拟地址空间下的地址
la t0, trap_s
li t1, 0xffff
slli t1, t1, 16
li t2, 0xffdf
add t1, t1, t2
slli t1, t1, 16
li t2, 0x8000
add t1, t1, t2
slli t1, t1, 16
add t0, t1, t0
csrw stvec, t0
//设置栈指针,值为虚拟地址空间下的地址
la t0, stack_top
li t1, 0xffff
slli t1, t1, 16
li t2, 0xffdf
add t1, t1, t2
slli t1, t1, 16
li t2, 0x8000
add t1, t1, t2
slli t1, t1, 16
add sp, t1, t0
//进入start_kernel函数
la t0, start_kernel
li t1, 0xffff
slli t1, t1, 16
li t2, 0xffdf
add t1, t1, t2
slli t1, t1, 16
li t2, 0x8000
add t1, t1, t2
slli t1, t1, 16
add t0, t1, t0
jr t0
```

这里就不详细解释了，唯一要说明的是由于这里标签仍然是原来的地址，而不是建立完映射的虚拟地址，因此要手动计算虚拟地址。

##### 4.3.2  修改M模式下异常处理代码

```assembly
//设置mscratch值，用于Mmode上下文切换
	la t0, init_stack_top
	csrw mscratch, t0
	
//mscratch寄存器与sp交换
	csrrw	sp,mscratch,sp
```

#### 4.4 修改进程调度相关代码sched.c

```c
//创建0号进程
	task[0] = (struct task_struct*)(0xffffffe000010000);
	task[0]->state = TASK_RUNNING;
	task[0]->counter = 0;
	task[0]->priority = 5;
	task[0]->blocked = 0;
	task[0]->pid = 0;
	task[0]->thread.sp = 0xffffffe000011000;
	current = task[0];
```

如图，设置task的地址和sp的地址都为响应的虚拟地址。

最终结果如下图：

![1607837744393](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1607837744393.png)

可以看到进程调度正常，并且打印的地址也正常，可以确定完成了页表相关处理。

#### 4.5 完成对不同section的保护

##### 调整perm参数

```
ffffffe000002000 R _rodata
```

打印rodata地址，发现是从上述地址开始的，转换为原来的物理地址即为0x80002000，而rodata的长度为0x1000，于是在该地址之前（text部分）将perm设置为0xb(xwrv = 1011)，之后设置为0x3(xwrv = 0011)，再在0x80003000之后设置为0x7(xwrv = 0111)即可。

 **思考题**：**如何验证这些属性是否被成功保护**

可以用写一条汇编语言尝试往text部分的地址里写东西，还可以尝试往rodata部分的地址里写东西，或者跳转至这部分地址尝试执行代码。这样一定会发生page fault错误，也就可以验证属性是否被保护了。

##### 修改medeleg寄存器

```c
//设置medeleg，完成section保护
	li t0, 0xb000
	csrw medeleg, t0
```

通过设置medeleg寄存器的15\13\12位，将三种page fault委托给S模式。



## Lab5: RISC-V 64 用户模式

### 1 实验简介

进一步巩固课堂学习的页式内存管理以及虚拟内存的相关知识，在低地址空间映射用户态程序，并尝试编写简单的系统调用处理函数。

### 2 实验环境

Docker Image

### 3 背景知识

#### 3.1 User模式基础介绍

处理器具有两种不同的模式：用户模式和内核模式。在内核模式下，执行代码对底层硬件具有完整且不受限制的访问权限，它可以执行任何CPU指令并引用任何内存地址。在用户模式下，执行代码无法直接访问硬件，必须委托给系统提供的接口才能访问硬件或内存。处理器根据处理器上运行的代码类型在两种模式之间切换。应用程序以用户模式运行，而核心操作系统组件以内核模式运行。
当启动用户模式应用程序时，内核将为该应用程序创建一个进程，为应用程序提供了专用虚拟地址空间等资源。因为应用程序的虚拟地址空间是私有的，所以一个应用程序无法更改属于另一个应用程序的数据。每个应用程序都是独立运行的，如果一个应用程序崩溃，其他应用程序和操作系统不会受到影响。同时，用户模式应用程序可访问的虚拟地址空间也受到限制，在用户模式下无法访问操作系统的虚拟地址，可防止应用程序修改关键操作系统数据。

#### 3.2 系统调用约定

系统调用是用户态应用程序请求内核服务的一种方式。在RISC-V中，我们使用ecall 指令进行系统调用。当执行这条指令时处理器会提升特权模式，跳转到异常处理函数处理这条系统调用。
Linux中RISC-V相关的系统调用可以在.../include/asm-generic/unistd.h 中找到，syscall(2)手册页上对RISC-V架构上的调用说明进行了总结，系统调用参数使用a0 - a5，系统调用号使用a7，系统调用的返回值会被保存到a0, a1中。

#### 3.3 程序原理及流程过程（我自己的理解）

这个实验其实是将上一个实验(lab4)的四个进程改成用户态进程，并给定一段代码，让这四个进程以给定的代码在用户态下运行。实际流程如下：

前面部分都与实验4一致，一开始进行内核的初始化，进入S模式下的start_kernel，初始化内核进程等等。之后由于其他几个进程使用的是用户态，因此需要给这几个进程单独创建页表映射。之后就进入内核下的main内的死循环函数，等待时钟中断。

当时钟中断发生的时候，依然经由trap_m，委托给S模式，handler_s()处理，进入do_timer()进行进程调度。

在进程调度的内是这个实验与前面实验最大的差别，前面的实验是经由do_timer()，schedule(),switch_to()等一系列函数，将进程修改，最终进入一个dead_loop()死循环函数等待下一次时钟中断发生，进行下一次调度。然而现在的情况是经由这一系列进行进程状态切换，上下文保存等工作，然后通过sret进入用户态，并跳转至用户态程序入口，运行用户态程序。

用户态程序中有两个ecall指令，触发ecall指令之后用户态会回到内核态，进入trap_s函数(使用了异常委托)处理这两个系统调用，在处理结束，trap_s经由sret正常返回用户态(sepc+4)，用户态程序继续执行，直到发生下一个时钟中断。

发生时钟中断之后，会强制进入M模式的trap_m处理中断，然后进入之前实验3的流程，尝试进行下一次进程调度。

### 4 实验步骤

#### 4.1 环境搭建

##### 4.1.1 建立映射

##### 4.1.2 组织文件结构

由于这次实验是在老师给出的lab4参考代码的基础上改的，因此组织文件结构与要求的略有差别。

![1610000095847](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1610000095847.png)

![1610000107286](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1610000107286.png)

#### 4.2 添加系统调用处理函数

syscall.h:

```c
#ifndef SYSCALL_H
#define SYSCALL_H

#include "sched.h"
typedef unsigned long int	uintptr_t;

uintptr_t sys_write(unsigned int fd, const char* buf, size_t count);
uintptr_t sys_getpid();

void handler_s(size_t scause, size_t sepc, uintptr_t* regs);

#endif
```

系统调用处理函数在strap.c中实现，其中handler_s()实现对于S模式下中断:

```c
void handler_s(size_t scause, size_t sepc, uintptr_t* regs){
//	printf("In handler_s, scause:%lx, sepc:%lx\n", scause, sepc);
	asm volatile ("add t1, zero, zero");
	if (scause >> 63) {		// interrupt
		
		if ( ( (scause << 1) >> 1 ) == 5 ) {	// supervisor timer interrupt
			asm volatile("ecall");
			do_timer();
		}
		return;
	}else if(scause >> 63 == 0){	//exception
		if(scause == 8){
			asm volatile("	li t0, 0x40000 \n \
					csrs sstatus, t0");
		//	printf("ecall from U in handler.\n");	
			size_t a7 = (size_t)*(regs + 17);
			size_t a0;
		//	a7 = 64;
		//	printf("a7:%lx\n", a7);
			if(a7 == 64){
			//	printf("a7 is 64\n");
			//	printf("a0, a1, a2: %lx, %lx, %lx", (size_t)*(regs + 10),(size_t)*(regs + 11),(size_t)*(regs + 12));
				unsigned int fd = (unsigned int)*(regs + 10);
				const char* buf = (char*)*(regs + 11);
				size_t count = (size_t)*(regs + 12);
			//	printf("%s", buf);
			//	unsigned int fd = 1;
			//	const char* buf = "[User] pid: ";
			//	size_t count = 12;
				a0 = sys_write(fd, buf, count);
			}else if(a7 == 172){
				a0 = sys_getpid();
			//	printf("[User] pid:%d\n", a0);
			}
			asm volatile ("addi t1, zero, 4");
			asm volatile ("add a0, zero, %0" :: "rK"(a0));
			return;
		}else if(scause == 12){
			printf("Instruction page fault on 0x%lx.\n", sepc);
		}else if(scause == 13){
			printf("Load page fault on 0x%lx.\n", sepc);
		}else if(scause == 15){
			printf("Store page fault on 0x%lx.\n", sepc);
		}else{
			printf("Unkonw page fault on 0x%lx.\n", sepc);
		}
	}	
}
```

handler_s首先判断scause是否为0x08，如果是说明本次异常是一个系统调用，之后检查a7寄存器，如果值为64，说明这是一个sys_write系统调用，于是将参数设定好并调用该系统调用。如果a7值为172，则说明这是一个sys_getpid系统调用，于是调用改系统调用，这两个系统调用函数都会以c语言返回值的方式返回返回值，我们手动用内联汇编的方式将返回值写进a0寄存器，并返回trap_s函数，trap_s函数将会把a0寄存器返回给用户态程序，这里就不详细写了。

sys_write系统调用：

```c
uintptr_t sys_write(unsigned int fd, const char* buf, size_t count){
//	printf("in sys_write");
	int i;
	for(i=0; i<count; i++){
//	printf("%d, %lx\n",i, *(buf+i));
		if(*(buf+i) == '\0')	break;
	//	printf("in loop");
		//	break;
	//	printf("%c", *(buf+i));
	//	puts(buf);
		putchar(*(buf+i));
	}
	return i;

//	uintptr_t num = (uintptr_t)printf("%s", buf);
//	return num;
}
```

这个函数将从地址buf开始的一系列字符打印到标准输出上，并返回打印字符的数量。从我无数的注释也可以看出我为了调这个函数费劲心机......

sys_getpid系统调用：

```c
extern struct task_struct *current;
uintptr_t sys_getpid(){
	return (uintptr_t)current->pid;
}
```

获得当前进程的pid。

#### 4.3 修改进程初始化以及进程调度相关逻辑

##### task_struct的修改

```c
/* 进程数据结构 */
struct task_struct {
     long state;    // 进程状态 Lab3中进程初始化时置为TASK_RUNNING
     long counter;  // 运行剩余时间 
     long priority; // 运行优先级 1最高 5最低
     long blocked;
     long pid;      // 进程标识符
     
    // Above Size Cost: 40 bytes

    struct thread_struct thread; // 该进程状态段
    size_t sepc;	//保存的sepc
    size_t sscratch;	//保存的sscratch
    pagetable_t pgtbl;
//    struct  mm_struct *mm;	//虚拟内存映射相关
};
```

这是我最后的task struct结构体，新增了 三个变量，分别是sepc，sscratch，pgtbl。

其中，sepc其实没什么用，sepc理论上应该保存的是用户态进程的中断位置，但是问题是如果保存了用户态进程的中断位置，实际上通过单步调试可以看到最终用户态程序会进入一个死循环，如果保存在这个位置那么下一次调度进入用户态进程就会继续跑这个死循环，而不会输出我们想要的[User]pid的哪一个字符串了，最终我并没有使用这个变量，而是在进程调度的时候，在每次即将进入用户态的时候将sepc设为0x0，然后通过sret进入用户程序，相当于每次都将进程重启，这也是用户态程序设计成这样的意义。

sscratch变量的意义是，它一开始要保存的是给用户进程用户态程序分配的用户栈，不同于task_struct->thread.sp的是，sp里面保存的是在内核态下的内核栈，是给S模式使用的，而sscratch中保存的是sp，在进入用户态之前会将sscratch变量与sscratch寄存器交换，sscratch得到了栈地址之后再与sp进行交换，这样在最后进入用户态的时候sp将保存着用户态的栈顶地址。

pgtbl用来保存页表地址，对于每一个用户态的进程，我们都需要给其建立页表，然而尽管页表内容是一致的(内核全部一致，用户态由于代码一样，映射位置也一样，所以其实内容一样)，但页表的位置是不一样的，页表首地址也不一样，因此需要存储。这里不适用struct mm_struct的原因是我觉得这里只需要存储一个pgtbl即可，同时mm_struct如果要使用还需要定于malloc()函数，比较麻烦。

##### 内存空间分配

```c
pagetable_t user_kvminit(){
//  upgtbl = 0x080ff2000;
  kalloc();
  pagetable_t upgtbl = (pagetable_t) kalloc();
//  printf("upgtbl:%x", upgtbl);
  memset(upgtbl, 0, PAGE_SIZE);
    
  //将内核页表复制到用户态页表（重做）
  // map devices
  uint64 uart = PA2VA(get_device_addr(UART_MMIO));
  kvmmap(upgtbl, uart, VA2PA(uart), get_device_size(UART_MMIO), (PTE_R | PTE_W));

  uint64 poweroff = PA2VA(get_device_addr(POWEROFF_MMIO));
  kvmmap(upgtbl, poweroff, VA2PA(poweroff), get_device_size(POWEROFF_MMIO), (PTE_R | PTE_W));

  // map kernel text executable and read-only.
  kvmmap(upgtbl, (uint64)&text_start, VA2PA((uint64)&text_start), (uint64)&text_end - (uint64)&text_start, (PTE_R | PTE_X));
  // map kernel data and the physical RAM we'll make use of.
  kvmmap(upgtbl, (uint64)&rodata_start, VA2PA((uint64)&rodata_start), (uint64)&rodata_end - (uint64)&rodata_start, (PTE_R));
  kvmmap(upgtbl, (uint64)&data_start, VA2PA((uint64)&data_start), (uint64)PHY_END - VA2PA((uint64)&data_start), (PTE_R | PTE_W));

//  memset(upgtbl, 0, PAGE_SIZE);
//  memcpy(upgtbl, kpgtbl, PAGE_SIZE * 13);	
  //给用户态程序构建页表
  kvmmap(upgtbl, 0x0, 0x084000000, 0x08000000, (PTE_W | PTE_R | PTE_X | PTE_U));
  
  return upgtbl;
//  write_csr(satp, MAKE_SATP(kpgtbl));
//  asm volatile("sfence.vma");
}
```

我首先在vm.c中定义了一个函数user_kvminit()如上，这个函数做的是，首先使用kalloc()获得一个页表地址，然后在这个页表地址出先重做一遍kvminit()里的内容(即设定好内核部分映射)，之后再次建立映射，将用户态程序从0x8400,0000的位置映射到虚拟地址0x0处。映射大小为0x0800,0000，权限如上，这里重要的是要写PTE_U，表明这里是U模式的地址，最后返回这个页表首地址。这个函数将会在task_init()被调用。

```c
task[i]->pgtbl = user_kvminit();
```

这样就把对应进程的页表建立好了。

##### 用户栈与内核栈

可以注意到我在上面的函数内没有做用户态栈的设置，整个0x0800,0000的空间全分给了用户程序，这是因为我忘了。不过没关系，我之后在task_init()函数中做了这件事。

```c
        task[i]->sscratch = (unsigned long long)alloc_page() + PAGE_SIZE;
        memset(task[i]->sscratch, 0, PAGE_SIZE);
        kvmmap(task[i]->pgtbl, (uint64)0xffffffdf80000000-PAGE_SIZE, VA2PA(task[i]->sscratch - PAGE_SIZE), (uint64)PAGE_SIZE, (PTE_R | PTE_W | PTE_X | PTE_U));
        task[i]->sscratch = (unsigned long long)0xffffffdf80000000;
```

我在task_init()函数中，首先使用allock_page()拿到了一页空闲的地址，然后使用kvmmap()将这一页映射到了0xffffffdf80000000处，即从0xffffffdf80000000开始往前一页的部分是用户的栈，之后非常重要的就是将这个栈顶的虚拟地址交给用户结构的额sscratch保存。

由于我们在创建页表映射的过程中必须使用物理地址，因我手动在之前关闭了satp，在所有这一切页表的工作完成之后，我们终于可以开启satp，一开始的satp需要赋值为内核态的页表地址，如下。

```c
    write_csr(satp, MAKE_SATP(kpgtbl));
    asm volatile("sfence.vma");
```

设立了用户栈之后，我们应该如何使用用户栈呢？我的实现方法是，在进入用户态(sret)之前，将sp与sscratch，此时用户态得到了用户栈地址，同时内核栈地址被保存在了sscratch中，然后在用户态发生ecall的时候，进入trap_s，此时切成了内核态，因此需要sp置位内核栈地址，我们可以再将sp与sscratch交换，然后在离开trap_s的时候换回来，如下。

```assembly
 	.globl __sret
__sret:
	call switch_to_user
	li t0, 0x100
	csrc sstatus, t0
	csrrw sp, sscratch, sp
	add t0, zero, zero
	csrw sepc, t0
	sret
```

```assembly
trap_s:

	# To set sscratch
	csrrw sp, sscratch, sp
	
	......
	
	
	csrrw sp, sscratch, sp
	sret

```

当然，进入用户态不仅仅是栈需要切换，还需要切换satp(由内核态的页表切换至对应用户态程序的页表)，这项工作我放在了switch_to_user函数中进行，这里就不详细写了。还有，我们想要进入用户态需要将sstatus的SPP置为0，这样再使用sret指令就可以跳转到用户态。

#### 4.4 测试程序结果

如下图，可以看到结果是正确的，具体我就不详细解释了。

![1610003053794](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\1610003053794.png)



































